{"cells":[{"cell_type":"markdown","id":"88e3d8a4","metadata":{"id":"88e3d8a4"},"source":["# EEG DataLoader"]},{"cell_type":"markdown","id":"78e016ab","metadata":{"id":"78e016ab"},"source":["The dimensions of the training set are as follows: 4,500 samples, 64 channels, and a time length of 795. This corresponds to 5 categories in y_train.\n","\n","The dimensions of the testing set are as follows: 750 samples, 64 channels, and a time length of 795. This corresponds to 5 categories in y_test.\n","\n","You can download it from this Google Drive link: [https://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing].\n","Please download the data and place it in your data_path at \"./data.\""]},{"cell_type":"code","execution_count":1,"id":"b284602d","metadata":{"id":"b284602d"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import matplotlib.pyplot as plt\n","from torchviz import make_dot"]},{"cell_type":"code","execution_count":2,"id":"fef2ddad","metadata":{"id":"fef2ddad"},"outputs":[],"source":["data_path = '/Users/siddhartharaovs/Downloads/CS555-Agile project Files/BCI_project/'"]},{"cell_type":"code","execution_count":3,"id":"fbb56c77","metadata":{"id":"fbb56c77"},"outputs":[],"source":["train_data = np.load(data_path + 'train_data.npy')\n","test_data = np.load(data_path + 'test_data.npy')\n","train_label = np.load(data_path + 'train_label.npy')\n","test_label = np.load(data_path + 'test_label.npy')\n","\n","#To convert the data into PyTorch tensors\n","x_train_tensor = torch.Tensor(train_data)\n","y_train_tensor = torch.LongTensor(train_label)\n","x_test_tensor = torch.Tensor(test_data)\n","y_test_tensor = torch.LongTensor(test_label)"]},{"cell_type":"code","execution_count":4,"id":"f13109bc","metadata":{"id":"f13109bc"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Setting GPU on your computer"]},{"cell_type":"code","execution_count":5,"id":"38430b1f","metadata":{"id":"38430b1f"},"outputs":[],"source":["train_dataset = TensorDataset(x_train_tensor.to(device), y_train_tensor.to(device)) # input data to Tensor dataloader\n","train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True, shuffle=True) #  Batch size refers to the number of data sample\n","test_dataset = TensorDataset(x_test_tensor.to(device), y_test_tensor.to(device))\n","test_loader = DataLoader(test_dataset, batch_size=64,  drop_last=True,shuffle=False)"]},{"cell_type":"markdown","id":"a304b4d6","metadata":{"id":"a304b4d6"},"source":["# Build simple Deep learning model"]},{"cell_type":"code","execution_count":7,"id":"2f9468f8","metadata":{"id":"2f9468f8"},"outputs":[],"source":["class ImprovedEEGClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ImprovedEEGClassifier, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2),\n","            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 99, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.LogSoftmax(dim=1)\n","        )\n","        self.batch_norm = nn.BatchNorm1d(512)\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.batch_norm(x)\n","        x = self.classifier(x)\n","        return x\n"]},{"cell_type":"code","execution_count":9,"id":"2369fa9c","metadata":{"id":"2369fa9c"},"outputs":[],"source":["num_classes = 5 # setting final output class\n","model = ImprovedEEGClassifier(num_classes).to(device)\n","criterion = nn.NLLLoss() # Use NLLLoss function to optimize\n","optimizer = optim.Adam(model.parameters(), lr=0.0001) # Setting parameters learning rate = 0.001"]},{"cell_type":"code","execution_count":10,"id":"086a7e33","metadata":{"id":"086a7e33","outputId":"17320448-2c7e-48a8-8c2f-c4d6ab88ef4e"},"outputs":[{"ename":"RuntimeError","evalue":"running_mean should contain 101376 elements not 512","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m      8\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mImprovedEEGClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:2482\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2480\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 101376 elements not 512"]}],"source":["num_epochs = 25 # setting training epochs (Number of training iterations)\n","for epoch in range(num_epochs):\n","    model.train()\n","    for data, labels in train_loader:\n","        #optimizer.zero_grad()\n","        outputs = model(data)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')"]},{"cell_type":"code","execution_count":24,"id":"7969f355","metadata":{"id":"7969f355","outputId":"8d6684b2-d2d3-452a-859a-5e464aeddefa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 19.89%\n"]}],"source":["model.eval() # Evaluate your model\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for data, labels in test_loader:\n","        outputs = model(data)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = correct / total\n","print(f'Test Accuracy: {accuracy * 100:.2f}%')"]},{"cell_type":"code","execution_count":20,"id":"f3af8a7c","metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 800x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.figure(figsize=(8, 6))\n","plt.bar([\"Test Accuracy\"], [accuracy * 100], color='blue')\n","plt.ylabel('Accuracy (%)')\n","plt.title('Test Accuracy')\n","plt.ylim(0, 100)\n","plt.show()\n"]},{"cell_type":"code","execution_count":21,"id":"507f4e57","metadata":{},"outputs":[{"ename":"ValueError","evalue":"Expected more than 1 value per channel when training, got input size torch.Size([1, 128])","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m EEGAutoencoderClassifier(num_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m795\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m graph \u001b[38;5;241m=\u001b[39m make_dot(output, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_parameters()))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Save the graph\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#graph.render(filename='model_graph', format='png')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display the graph\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mEEGAutoencoderClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Applying batch normalization before classifier\u001b[39;00m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:2480\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2468\u001b[0m         batch_norm,\n\u001b[1;32m   2469\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2477\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2478\u001b[0m     )\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2480\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2484\u001b[0m )\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2446\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 128])"]}],"source":["model = EEGAutoencoderClassifier(num_classes).to(device)\n","dummy_input = torch.randn(1, 64 * 795).to(device)\n","output = model(dummy_input)\n","graph = make_dot(output, params=dict(model.named_parameters()))\n","\n","#Save the graph\n","#graph.render(filename='model_graph', format='png')\n","# Display the graph\n","graph.view()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}
